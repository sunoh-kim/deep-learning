{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M2177.003100 Deep Learning <br> Assignment #1 Part 3: Playing with Neural Networks by TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (C) Data Science & AI Laboratory, Seoul National University. This material is for educational uses only. Some contents are based on the material provided by other paper/book authors and may be copyrighted by them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Previously in `Assignment2-1_Data_Curation.ipynb`, we created a pickle with formatted datasets for training, development and testing on the [notMNIST dataset](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html).\n",
    "\n",
    "The goal of this assignment is to progressively train deeper and more accurate models using TensorFlow.\n",
    "\n",
    "**Note**: certain details are missing or ambiguous on purpose, in order to test your knowledge on the related materials. However, if you really feel that something essential is missing and cannot proceed to the next step, then contact the teaching staff with clear description of your problem.\n",
    "\n",
    "### Submitting your work:\n",
    "<font color=red>**DO NOT clear the final outputs**</font> so that TAs can grade both your code and results.  \n",
    "Once you have done **part 1 - 3**, run the *CollectSubmission.sh* script with your **Student number** as input argument. <br>\n",
    "This will produce a compressed file called *[Your student number].tar.gz*. Please submit this file on ETL. &nbsp;&nbsp; (Usage: ./*CollectSubmission.sh* &nbsp; 20\\*\\*-\\*\\*\\*\\*\\*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets\n",
    "\n",
    "First reload the data we generated in `Assignment2-1_Data_Curation.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "import os\n",
    "\n",
    "#configuration for gpu usage\n",
    "conf = tf.ConfigProto()\n",
    "# you can modify below as you want\n",
    "#conf.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "#conf.gpu_options.allow_growth = True\n",
    "#os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19456,
     "status": "ok",
     "timestamp": 1449847956073,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "0ddb1607-1fc4-4ddb-de28-6c7ab7fb0c33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'data/notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19723,
     "status": "ok",
     "timestamp": 1449847956364,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "2ba0fc75-1487-4ace-a562-cf81cae82793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "    # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nCLVqyQ5vPPH"
   },
   "source": [
    "## TensorFlow tutorial: Fully Connected Network\n",
    "\n",
    "We're first going to train a **fully connected network** with *1 hidden layer* with *1024 units* using stochastic gradient descent (SGD).\n",
    "\n",
    "TensorFlow works like this:\n",
    "* First you describe the computation that you want to see performed: what the inputs, the variables, and the operations look like. These get created as nodes over a computation graph. This description is all contained within the block below:\n",
    "\n",
    "      with graph.as_default():\n",
    "          ...\n",
    "\n",
    "* Then you can run the operations on this graph as many times as you want by calling `session.run()`, providing it outputs to fetch from the graph that get returned. This runtime operation is all contained in the block below:\n",
    "\n",
    "      with tf.Session(graph=graph) as session:\n",
    "          ...\n",
    "\n",
    "Let's load all the data into TensorFlow and build the computation graph corresponding to our training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "nn_hidden = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_dataset = tf.placeholder(tf.float32,\n",
    "                                      shape=(None, image_size * image_size))\n",
    "    tf_labels = tf.placeholder(tf.float32, shape=(None, num_labels))\n",
    "    \n",
    "    # Parameters. \n",
    "    w1 = tf.Variable(tf.truncated_normal([image_size * image_size, nn_hidden]))\n",
    "    b1 = tf.Variable(tf.zeros([nn_hidden]))\n",
    "    w2 = tf.Variable(tf.truncated_normal([nn_hidden, num_labels]))\n",
    "    b2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    # Training computation.\n",
    "    hidden = tf.tanh(tf.matmul(tf_dataset, w1) + b1)\n",
    "    logits = tf.matmul(hidden, w2) + b2\n",
    "    \n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_labels, logits=logits))\n",
    "    \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    prediction = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KQcL4uqISHjP"
   },
   "source": [
    "Let's run this computation and iterate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 41.872452\n",
      "Minibatch accuracy: 13.3%\n",
      "Validation accuracy: 24.6%\n",
      "Minibatch loss at step 1000: 4.126708\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 79.7%\n",
      "Minibatch loss at step 2000: 2.639631\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 80.3%\n",
      "Minibatch loss at step 3000: 1.558147\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.1%\n",
      "Minibatch loss at step 4000: 1.605194\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 80.4%\n",
      "Minibatch loss at step 5000: 0.928094\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 75.7%\n",
      "Minibatch loss at step 6000: 1.161180\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 82.9%\n",
      "Minibatch loss at step 7000: 1.067366\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 78.3%\n",
      "Minibatch loss at step 8000: 0.622747\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 82.6%\n",
      "Minibatch loss at step 9000: 0.999509\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.4%\n",
      "Test accuracy: 90.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10000\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.equal(np.argmax(predictions, 1), np.argmax(labels, 1)))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict_train={tf_dataset: batch_data, tf_labels: batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, prediction], feed_dict=feed_dict_train)\n",
    "        if (step % 1000 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            valid_prediction = session.run(logits, feed_dict={tf_dataset: valid_dataset})\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction, valid_labels))\n",
    "                  \n",
    "    test_prediction = session.run(prediction, feed_dict={tf_dataset: test_dataset})\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction, test_labels))\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(session, \"./model_checkpoints/my_model_final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, you have built the model in a naive way. However, TensorFlow provides a module named tf.layers for your convenience. \n",
    "\n",
    "From now on, build the same model as above using layers module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_l=tf.Graph()\n",
    "with graph_l.as_default():\n",
    "    tf_dataset_l=tf.placeholder(tf.float32, shape=(None, image_size * image_size))\n",
    "    tf_labels_l=tf.placeholder(tf.float32, shape=(None, num_labels))\n",
    "    \n",
    "    #neural network consists of two lines\n",
    "    dense = tf.layers.dense(tf_dataset_l, nn_hidden, activation=tf.tanh)\n",
    "    logits_l = tf.layers.dense(dense, num_labels, activation=None)\n",
    "    \n",
    "    #Loss\n",
    "    loss_l = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_labels_l, logits=logits_l))\n",
    "    \n",
    "    #Optimizer\n",
    "    optimizer_l = tf.train.GradientDescentOptimizer(0.5).minimize(loss_l)\n",
    "    \n",
    "    #Predictions for the training\n",
    "    prediction_l = tf.nn.softmax(logits_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.514579\n",
      "Validation accuracy: 51.5%\n",
      "Minibatch loss at step 1000: 0.700303\n",
      "Validation accuracy: 83.7%\n",
      "Minibatch loss at step 2000: 0.577286\n",
      "Validation accuracy: 85.3%\n",
      "Minibatch loss at step 3000: 0.474981\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 4000: 0.471527\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 5000: 0.321932\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 6000: 0.521421\n",
      "Validation accuracy: 87.9%\n",
      "Minibatch loss at step 7000: 0.381020\n",
      "Validation accuracy: 87.9%\n",
      "Minibatch loss at step 8000: 0.258412\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 9000: 0.452011\n",
      "Validation accuracy: 87.6%\n",
      "Test accuracy: 94.0%\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph_l, config=conf) as session_l:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :].astype(float)\n",
    "        feed_dict_l = {tf_dataset_l: batch_data, tf_labels_l: batch_labels}\n",
    "        _, l_l, predictions_l = session_l.run([optimizer_l, loss_l, prediction_l], feed_dict=feed_dict_l)\n",
    "        if(step % 1000 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l_l))\n",
    "            feed_dict_val_l = {tf_dataset_l: valid_dataset}\n",
    "            valid_prediction_l = session_l.run(prediction_l, feed_dict={tf_dataset_l: valid_dataset, tf_labels_l: valid_labels})\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction_l, valid_labels))\n",
    "\n",
    "    feed_dict_test_l = {tf_dataset_l: test_dataset}\n",
    "    test_prediction_l = session_l.run(prediction_l, feed_dict=feed_dict_test_l)\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction_l, test_labels))\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(session_l, \"./model_checkpoints/my_model_final_using_layers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Problem 1\n",
    "-------\n",
    "\n",
    "**Describe below** why there is a difference in an accuracy between the graph using layer module and the graph which is built in a naive way. **explain simply**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe here\n",
    "\n",
    "TensorFlow makes it possible to build and train complex neural networks across thousands of multi-GPU servers. I guess the graph using layer module adopts more effective strategies for learning than the graph which is built in a naive way. For example, exploiting real Gradient descent requires a lot of skills while it's theory is easy to understand. So, the graph using layer module in TensorFlow offers effective learning methods for better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7omWxtvLLxik"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "-------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! (It doesn't matter whether you implement it in a naive way or using layer module. HOWEVER, you CANNOT use other type of layers such as conv.) \n",
    "\n",
    "The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.kr/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595). You may use techniques below.\n",
    "\n",
    "1. Experiment with different hyperparameters: num_steps, learning rate, etc.\n",
    "2. We used a fixed learning rate epsilon for gradient descent. Implement an annealing schedule for the gradient descent learning rate ([more info](http://cs231n.github.io/neural-networks-3/#anneal)). *Hint*. Try using `tf.train.exponential_decay`.    \n",
    "3. We used a tanh activation function for our hidden layer. Experiment with other activation functions included in TensorFlow.\n",
    "4. Extend the network to multiple hidden layers. Experiment with the layer sizes. Adding another hidden layer means you will need to adjust the code. \n",
    "5. Introduce and tune regularization method (e.g. L2 regularization) for your model. Remeber that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should imporve your validation / test accuracy.\n",
    "6. Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides nn.dropout() for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "**Evaluation:** You will get full credit if your best test accuracy exceeds <font color=red>$93\\%$</font>. Save your best perfoming model as my_model_final using saver. (Refer to the cell above) \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " TODO \n",
      "Initialized\n",
      "Minibatch loss at step 0: 3.068878\n",
      "Minibatch accuracy: 6.1%\n",
      "Validation accuracy: 20.4%\n",
      "Minibatch loss at step 1000: 0.861525\n",
      "Minibatch accuracy: 91.0%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 2000: 0.848564\n",
      "Minibatch accuracy: 90.2%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 3000: 0.694554\n",
      "Minibatch accuracy: 93.4%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 4000: 0.667068\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 5000: 0.606312\n",
      "Minibatch accuracy: 95.5%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 6000: 0.563053\n",
      "Minibatch accuracy: 94.9%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 7000: 0.535996\n",
      "Minibatch accuracy: 95.5%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 8000: 0.475382\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 9000: 0.449011\n",
      "Minibatch accuracy: 96.5%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 10000: 0.433286\n",
      "Minibatch accuracy: 97.3%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 11000: 0.402410\n",
      "Minibatch accuracy: 96.5%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 12000: 0.341323\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 13000: 0.345428\n",
      "Minibatch accuracy: 97.5%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 14000: 0.341863\n",
      "Minibatch accuracy: 97.5%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 15000: 0.307677\n",
      "Minibatch accuracy: 98.8%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 16000: 0.326750\n",
      "Minibatch accuracy: 97.5%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 17000: 0.285594\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 18000: 0.268804\n",
      "Minibatch accuracy: 98.6%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 19000: 0.274742\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 20000: 0.275347\n",
      "Minibatch accuracy: 97.3%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 21000: 0.261844\n",
      "Minibatch accuracy: 98.0%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 22000: 0.239827\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 23000: 0.234752\n",
      "Minibatch accuracy: 98.6%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 24000: 0.232421\n",
      "Minibatch accuracy: 98.2%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 25000: 0.239156\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 26000: 0.199879\n",
      "Minibatch accuracy: 98.8%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 27000: 0.227787\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 28000: 0.225958\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 29000: 0.193658\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 91.2%\n",
      "Test accuracy: 96.0%\n"
     ]
    }
   ],
   "source": [
    "print(__doc__)\n",
    "\"\"\" TODO \"\"\"\n",
    "num_steps = 30000\n",
    "batch_size = 512\n",
    "reg_lambda = 0.0001\n",
    "nn_hidden1 = 4096\n",
    "nn_hidden2 = int(nn_hidden1 * 0.5)\n",
    "nn_hidden3 = int(nn_hidden2 * 0.5)\n",
    "nn_hidden4 = int(nn_hidden3 * 0.5)\n",
    "nn_hidden5 = int(nn_hidden4 * 0.5)\n",
    "dropout_prob = 0.8\n",
    "\n",
    "my_graph = tf.Graph()\n",
    "with my_graph.as_default():\n",
    "    \n",
    "    tf_dataset = tf.placeholder(tf.float32,\n",
    "                                      shape=(None, image_size * image_size))\n",
    "    tf_labels = tf.placeholder(tf.float32, shape=(None, num_labels))\n",
    "    \n",
    "    W1 = tf.Variable(tf.truncated_normal([image_size * image_size, nn_hidden1], stddev=tf.math.sqrt(2.0/(image_size*image_size))))\n",
    "    b1 = tf.Variable(tf.zeros([nn_hidden1]))\n",
    "    W2 = tf.Variable(tf.truncated_normal([nn_hidden1, nn_hidden2], stddev=tf.math.sqrt(2.0/nn_hidden1)))\n",
    "    b2 = tf.Variable(tf.zeros([nn_hidden2]))\n",
    "    W3 = tf.Variable(tf.truncated_normal([nn_hidden2, nn_hidden3], stddev=tf.math.sqrt(2.0/nn_hidden2)))\n",
    "    b3 = tf.Variable(tf.zeros([nn_hidden3]))\n",
    "    W4 = tf.Variable(tf.truncated_normal([nn_hidden3, nn_hidden4], stddev=tf.math.sqrt(2.0/nn_hidden3)))\n",
    "    b4 = tf.Variable(tf.zeros([nn_hidden4]))\n",
    "    W5 = tf.Variable(tf.truncated_normal([nn_hidden4, nn_hidden5], stddev=tf.math.sqrt(2.0/nn_hidden4)))\n",
    "    b5 = tf.Variable(tf.zeros([nn_hidden5]))\n",
    "    W6 = tf.Variable(tf.truncated_normal([nn_hidden5, num_labels], stddev=tf.math.sqrt(2.0/nn_hidden5)))\n",
    "    b6 = tf.Variable(tf.zeros([num_labels]))\n",
    "        \n",
    "    logits1 = tf.matmul(tf_dataset, W1) + b1\n",
    "    hidden1 = tf.nn.relu(logits1)\n",
    "    # Dropout\n",
    "    hidden1_dropout = tf.nn.dropout(hidden1, dropout_prob)\n",
    "    \n",
    "    logits2 = tf.matmul(hidden1_dropout, W2) + b2\n",
    "    hidden2 = tf.nn.relu(logits2)\n",
    "    hidden2_dropout = tf.nn.dropout(hidden2, dropout_prob)\n",
    "    \n",
    "    logits3 = tf.matmul(hidden2_dropout, W3) + b3\n",
    "    hidden3 = tf.nn.relu(logits3)\n",
    "    hidden3_dropout = tf.nn.dropout(hidden3, dropout_prob)\n",
    "\n",
    "    logits4 = tf.matmul(hidden3_dropout, W4) + b4\n",
    "    hidden4 = tf.nn.relu(logits4)\n",
    "    hidden4_dropout = tf.nn.dropout(hidden4, dropout_prob)\n",
    "\n",
    "    logits5 = tf.matmul(hidden4_dropout, W5) + b5\n",
    "    hidden5 = tf.nn.relu(logits5)\n",
    "    hidden5_dropout = tf.nn.dropout(hidden5, dropout_prob)\n",
    "    \n",
    "    # Output\n",
    "    logits6 = tf.matmul(hidden5_dropout, W6) + b6 \n",
    "    \n",
    "    # Normal loss function\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_labels, logits=logits6))\n",
    "    # L2 Regularization\n",
    "    regularizers = tf.nn.l2_loss(W1) + tf.nn.l2_loss(W2) + \\\n",
    "                   tf.nn.l2_loss(W3) + tf.nn.l2_loss(W4) + \\\n",
    "                   tf.nn.l2_loss(W5) + tf.nn.l2_loss(W6)\n",
    "    loss = tf.reduce_mean(loss + reg_lambda * regularizers)\n",
    "\n",
    "    # Exponential decay\n",
    "    global_step = tf.Variable(0)\n",
    "    initial_learning_rate = 0.5\n",
    "    learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step, num_steps, 0.95, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    prediction = tf.nn.softmax(logits6)\n",
    "      \n",
    "\n",
    "with tf.Session(graph=my_graph, config=conf) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict_train={tf_dataset: batch_data, tf_labels: batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, prediction], feed_dict=feed_dict_train)\n",
    "        if (step % 1000 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            valid_prediction = session.run(prediction, feed_dict={tf_dataset: valid_dataset, tf_labels: valid_labels})\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction, valid_labels))\n",
    "                  \n",
    "    test_prediction = session.run(prediction, feed_dict={tf_dataset: test_dataset})\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction, test_labels))\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(session, \"./model_checkpoints/my_model_final\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "2_fullyconnected.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
