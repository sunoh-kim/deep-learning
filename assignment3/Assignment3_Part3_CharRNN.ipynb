{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M2177.003100 Deep Learning <br> Assignment #3 Part 3: Language Modeling with CharRNN\n",
    "\n",
    "Copyright (C) Data Science Laboratory, Seoul National University. This material is for educational uses only. Some contents are based on the material provided by other paper/book authors and may be copyrighted by them. Written by Sang-gil Lee, October 2018\n",
    "\n",
    "This is a character-level language model using recurrent neural networks (RNNs).\n",
    "It has become very popular as a starter kit for learning how RNN works in practice.\n",
    "\n",
    "Before we start, what is \"language modeling\" anyway? Intuitively, \"language modeling\" is teaching the model about a general probability distribution of our words and sentences.\n",
    "\n",
    "So we ask the model like: \"hey just say whatever words from your estimation of the wikipedia word distribution\", and the model responds like \"ok, i learned from wikipedia, and the most frequent word is \"the\". so let me start with \"the\". the wikipedia is blah blah blah\"\n",
    "\n",
    "Thus, by teaching the model to speak for itself, we can test the model's capability of learning temporal relationships between sequences.\n",
    "\n",
    "Original blog post & code:\n",
    "https://github.com/karpathy/char-rnn\n",
    "http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "But the original code is written in lua torch which looks less pretty :(\n",
    "\n",
    "There is a clean port of char-RNN in TensorFlow\n",
    "https://github.com/sherjilozair/char-rnn-tensorflow\n",
    "This iPython notebook is basically a copypasta of this repo.\n",
    "\n",
    "That said, you are allowed to copy paste the codes from the original repo.\n",
    "HOWEVER, <font color=red> try to implement the model yourself first </font>, and consider the original source code as a last resort.\n",
    "You will learn a lot while wrapping around your head during the implementation. And you will understand nuts and bolts of RNNs more clearly in a code level.\n",
    "\n",
    "### AND MOST IMPORTANTLY, IF YOU JUST BLINDLY COPY PASTE THE CODE, YOU SHALL RUIN YOUR EXAM.\n",
    "### The exam is designed to be solvable for students that actually have written the code themselves.\n",
    "At least strictly re-type the codes from the original repo line-by-line, and understand what each line means thoroughly.\n",
    "\n",
    "## YOU HAVE BEEN WARNED. :)\n",
    "\n",
    "\n",
    "\n",
    "### Submitting your work:\n",
    "<font color=red>**DO NOT clear the final outputs**</font> so that TAs can grade both your code and results.  \n",
    "Once you have done **all Assignment Part 1-5**, run the *CollectSubmission.sh* script with your **Team number** as input argument. <br>\n",
    "This will produce a zipped file called *[Your team number].zip*. Please submit this file on ETL. &nbsp;&nbsp; (Usage: ./*CollectSubmission.sh* team_#)\n",
    "\n",
    "### Character language modeling (20 points)\n",
    "\n",
    "This assignment is an on/off one: just make this notebook **\"work\"** without problem by: \n",
    "\n",
    "1. implementing **1. \\_\\_init\\_\\_()** and **2. sample()** of RNN **Model()** class from **char_rnn.py**\n",
    "\n",
    "2. briefly summarizing, at the end of the script, how you implmeneted the model & why you changed some other parts of the code. yes,  <font color=red> there are other intentional pitfalls inside the code </font>. just copy-pasting the \\_\\_init\\_\\_() will not work. can you tell me why?\n",
    "\n",
    "### The Definition of **\"work\"** is as follows:\n",
    "\n",
    "1. Training loss must be <font color=red> below 0.2 </font>. We will check the training loss output from the training code block. We don't split the data into train-valid-test. Don't forget to <font color=red> NOT clear the output from train(args)</font>, where the training loss will be printed! TA will check the logged output from train(args)\n",
    "\n",
    "2. calling sample(args.sample) at the last code block <font color=red> must generate some meaningful sentences </font>. The quality of the sentence does not count, unless the generated sentence is something like \"aaaaaaaaaaaaaabbbbbb\" or \"b\" u tlttfcwaU c  fGcnrh i.\\nh mt he!bsthpme\".\n",
    "\n",
    "\n",
    "\n",
    "Now proceed to the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ipython magic function for limiting the gpu to be seen for tensorflow\n",
    "# if you have just 1 GPU, specify the value to 0\n",
    "# if you have multiple GPUs (nut) and want to specify which GPU to use, specify this value to 0 or 1 or etc.\n",
    "%env CUDA_DEVICE_ORDER = PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pil-kso/anaconda3/envs/deep-learning-19/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/pil-kso/anaconda3/envs/deep-learning-19/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/pil-kso/anaconda3/envs/deep-learning-19/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/pil-kso/anaconda3/envs/deep-learning-19/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/pil-kso/anaconda3/envs/deep-learning-19/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/pil-kso/anaconda3/envs/deep-learning-19/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# load a bunch of libraries\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.contrib import legacy_seq2seq\n",
    "import numpy as np\n",
    "import argparse\n",
    "import time\n",
    "import os\n",
    "from six.moves import cPickle\n",
    "from six import text_type\n",
    "import sys\n",
    "\n",
    "# this module is from the utils.py file of this folder\n",
    "# it handles loading texts to digits (aka. tokens) which are recognizable for the model\n",
    "from utils import TextLoader\n",
    "\n",
    "# this module is from the char_rnn.py file of this folder\n",
    "# the task is implementing the CharRNN inside the class definition from this file\n",
    "from char_rnn import Model\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for TensorFlow vram efficiency: if this is not specified, the model hogs all the VRAM even if it's not necessary\n",
    "# bad & greedy TF! but it has a reason for this design choice FWIW, try googling it if interested\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(batch_size=500, data_dir='data/tinyshakespeare', decay_rate=0.97, grad_clip=5.0, init_from=None, input_keep_prob=1.0, learning_rate=0.002, model='lstm', num_epochs=50, num_layers=5, output_keep_prob=1.0, rnn_size=256, save_dir='models_char_rnn', save_every=1000, seq_length=50)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# argparsing\n",
    "parser = argparse.ArgumentParser(\n",
    "                    formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "# Data and model checkpoints directories\n",
    "parser.add_argument('--data_dir', type=str, default='data/tinyshakespeare',\n",
    "                    help='data directory containing input.txt with training examples')\n",
    "parser.add_argument('--save_dir', type=str, default='models_char_rnn',\n",
    "                    help='directory to store checkpointed models')\n",
    "parser.add_argument('--save_every', type=int, default=1000,\n",
    "                    help='Save frequency. Number of passes between checkpoints of the model.')\n",
    "parser.add_argument('--init_from', type=str, default=None,\n",
    "                    help=\"\"\"continue training from saved model at this path (usually \"save\").\n",
    "                        Path must contain files saved by previous training process:\n",
    "                        'config.pkl'        : configuration;\n",
    "                        'chars_vocab.pkl'   : vocabulary definitions;\n",
    "                        'checkpoint'        : paths to model file(s) (created by tf).\n",
    "                                              Note: this file contains absolute paths, be careful when moving files around;\n",
    "                        'model.ckpt-*'      : file(s) with model definition (created by tf)\n",
    "                         Model params must be the same between multiple runs (model, rnn_size, num_layers and seq_length).\n",
    "                    \"\"\")\n",
    "# Model params\n",
    "parser.add_argument('--model', type=str, default='lstm',\n",
    "                    help='lstm, rnn, gru, or nas')\n",
    "parser.add_argument('--rnn_size', type=int, default=256, # 128\n",
    "                    help='size of RNN hidden state')\n",
    "parser.add_argument('--num_layers', type=int, default=5, #5 2\n",
    "                    help='number of layers in the RNN')\n",
    "# Optimization\n",
    "parser.add_argument('--seq_length', type=int, default=50, #500 50\n",
    "                    help='RNN sequence length. Number of timesteps to unroll for.')\n",
    "parser.add_argument('--batch_size', type=int, default=500,#5 50 128\n",
    "                    help=\"\"\"minibatch size. Number of sequences propagated through the network in parallel.\n",
    "                            Pick batch-sizes to fully leverage the GPU (e.g. until the memory is filled up)\n",
    "                            commonly in the range 10-500.\"\"\")\n",
    "parser.add_argument('--num_epochs', type=int, default=50,\n",
    "                    help='number of epochs. Number of full passes through the training examples.')\n",
    "parser.add_argument('--grad_clip', type=float, default=5., #20.\n",
    "                    help='clip gradients at this value')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.002, #0.01 0.002\n",
    "                    help='learning rate')\n",
    "parser.add_argument('--decay_rate', type=float, default=0.97,#0.70 0.97\n",
    "                    help='decay rate for rmsprop')\n",
    "parser.add_argument('--output_keep_prob', type=float, default=1.0, #0.1\n",
    "                    help='probability of keeping weights in the hidden layer')\n",
    "parser.add_argument('--input_keep_prob', type=float, default=1.0, #0.1\n",
    "                    help='probability of keeping weights in the input layer')\n",
    "\n",
    "# needed for argparsing within jupyter notebook\n",
    "# https://stackoverflow.com/questions/30656777/how-to-call-module-written-with-argparse-in-ipython-notebook\n",
    "sys.argv = ['-f']\n",
    "args = parser.parse_args()\n",
    "\n",
    "# print args: see if the hyperparemeters look pretty to you\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading preprocessed files\n",
      "[[49  9  7 ...  1  4  7]\n",
      " [ 6 16  0 ...  2  6  0]\n",
      " [ 0 23  4 ... 12  0  4]\n",
      " ...\n",
      " [ 4  9  7 ...  1 11 11]\n",
      " [ 4 20  1 ...  3  7 20]\n",
      " [ 2  0 17 ...  7  0  6]]\n",
      "(500, 50)\n",
      "[[ 9  7  6 ...  4  7  0]\n",
      " [16  0 14 ...  6  0  4]\n",
      " [23  4  7 ...  0  4 18]\n",
      " ...\n",
      " [ 9  7 16 ... 11 11  0]\n",
      " [20  1  0 ...  7 20  3]\n",
      " [ 0 17  1 ...  0  6  2]]\n",
      "(500, 50)\n"
     ]
    }
   ],
   "source": [
    "# protip: always check the data and poke around the data yourself\n",
    "# you will get a lot of insights by looking at the data\n",
    "data_loader = TextLoader(args.data_dir, args.batch_size, args.seq_length)\n",
    "data_loader.reset_batch_pointer()\n",
    "\n",
    "x, y = data_loader.next_batch()\n",
    "\n",
    "# our data has a shape of (N, T), where N=batch_size and T=seq_length\n",
    "print(x)\n",
    "print(x.shape)\n",
    "print(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[49  9  7  6  2  0 37  9  2  9 57  1  8 24 10 43  1 18  3  7  1  0 17  1\n",
      "  0 23  7  3 19  1  1 12  0  4  8 15  0 18 13  7  2  5  1  7 16  0  5  1\n",
      "  4  7]\n",
      "[ 9  7  6  2  0 37  9  2  9 57  1  8 24 10 43  1 18  3  7  1  0 17  1  0\n",
      " 23  7  3 19  1  1 12  0  4  8 15  0 18 13  7  2  5  1  7 16  0  5  1  4\n",
      "  7  0]\n"
     ]
    }
   ],
   "source": [
    "# see what the first entry of the batch look like\n",
    "print(x[0])\n",
    "print(y[0])\n",
    "# y is just an x shifted to the left by one: so the network will predict the next token y given x. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop definition\n",
    "def train(args):\n",
    "    data_loader = TextLoader(args.data_dir, args.batch_size, args.seq_length)\n",
    "    args.vocab_size = data_loader.vocab_size\n",
    "    print(\"vocabulary size: \" + str(args.vocab_size))\n",
    "\n",
    "    # check compatibility if training is continued from previously saved model\n",
    "    if args.init_from is not None:\n",
    "        # check if all necessary files exist\n",
    "        assert os.path.isdir(args.init_from),\" %s must be a a path\" % args.init_from\n",
    "        assert os.path.isfile(os.path.join(args.init_from,\"config.pkl\")),\"config.pkl file does not exist in path %s\"%args.init_from\n",
    "        assert os.path.isfile(os.path.join(args.init_from,\"chars_vocab.pkl\")),\"chars_vocab.pkl.pkl file does not exist in path %s\" % args.init_from\n",
    "        ckpt = tf.train.latest_checkpoint(args.init_from)\n",
    "        assert ckpt, \"No checkpoint found\"\n",
    "\n",
    "        # open old config and check if models are compatible\n",
    "        with open(os.path.join(args.init_from, 'config.pkl'), 'rb') as f:\n",
    "            saved_model_args = cPickle.load(f)\n",
    "        need_be_same = [\"model\", \"rnn_size\", \"num_layers\", \"seq_length\"]\n",
    "        for checkme in need_be_same:\n",
    "            assert vars(saved_model_args)[checkme]==vars(args)[checkme],\"Command line argument and saved model disagree on '%s' \"%checkme\n",
    "\n",
    "        # open saved vocab/dict and check if vocabs/dicts are compatible\n",
    "        with open(os.path.join(args.init_from, 'chars_vocab.pkl'), 'rb') as f:\n",
    "            saved_chars, saved_vocab = cPickle.load(f)\n",
    "        assert saved_chars==data_loader.chars, \"Data and loaded model disagree on character set!\"\n",
    "        assert saved_vocab==data_loader.vocab, \"Data and loaded model disagree on dictionary mappings!\"\n",
    "\n",
    "    if not os.path.isdir(args.save_dir):\n",
    "        os.makedirs(args.save_dir)\n",
    "    with open(os.path.join(args.save_dir, 'config.pkl'), 'wb') as f:\n",
    "        cPickle.dump(args, f)\n",
    "    with open(os.path.join(args.save_dir, 'chars_vocab.pkl'), 'wb') as f:\n",
    "        cPickle.dump((data_loader.chars, data_loader.vocab), f)\n",
    "    \n",
    "    print(\"building the model... may take some time...\")\n",
    "    ##################### This line builds the CharRNN model defined in char_rnn.py #####################\n",
    "    model = Model(args)\n",
    "    print(\"model built! starting training...\")\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=1)\n",
    "        # restore model\n",
    "        if args.init_from is not None:\n",
    "            saver.restore(sess, ckpt)\n",
    "        for e in range(args.num_epochs):\n",
    "            sess.run(tf.assign(model.lr,\n",
    "                               args.learning_rate * (args.decay_rate ** e)))\n",
    "            data_loader.reset_batch_pointer()\n",
    "            state = sess.run(model.initial_state)\n",
    "            \n",
    "            for b in range(int(data_loader.num_batches)):\n",
    "                start = time.time()\n",
    "                x, y = data_loader.next_batch()\n",
    "                feed = {model.input_data: x, model.targets: y}\n",
    "                for i, (c, h) in enumerate(model.initial_state):\n",
    "                    feed[c] = state[i].c\n",
    "                    feed[h] = state[i].h\n",
    "\n",
    "                train_loss, state, _ = sess.run([model.cost, model.final_state, model.train_op], feed)\n",
    "\n",
    "                end = time.time()\n",
    "                \n",
    "                # print training log every 100 steps\n",
    "                if ((e * data_loader.num_batches + b) % 100 == 0):\n",
    "                    print(\"{}/{} (epoch {}), train_loss = {:.3f}, time/batch = {:.3f}\"\n",
    "                          .format(e * data_loader.num_batches + b,\n",
    "                                  args.num_epochs * data_loader.num_batches,\n",
    "                                  e, train_loss, end - start))\n",
    "                if (e * data_loader.num_batches + b) % args.save_every == 0\\\n",
    "                        or (e == args.num_epochs-1 and\n",
    "                            b == data_loader.num_batches-1):\n",
    "                    # save for the last result\n",
    "                    checkpoint_path = os.path.join(args.save_dir, 'model.ckpt')\n",
    "                    saver.save(sess, checkpoint_path,\n",
    "                               global_step=e * data_loader.num_batches + b)\n",
    "                    print(\"model saved to {}\".format(checkpoint_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading preprocessed files\n",
      "vocabulary size: 65\n",
      "building the model... may take some time...\n",
      "model built! starting training...\n",
      "0/2200 (epoch 0), train_loss = 4.204, time/batch = 1.107\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "100/2200 (epoch 2), train_loss = 3.301, time/batch = 0.122\n",
      "200/2200 (epoch 4), train_loss = 3.317, time/batch = 0.115\n",
      "300/2200 (epoch 6), train_loss = 2.600, time/batch = 0.116\n",
      "400/2200 (epoch 9), train_loss = 2.303, time/batch = 0.126\n",
      "500/2200 (epoch 11), train_loss = 2.043, time/batch = 0.118\n",
      "600/2200 (epoch 13), train_loss = 1.905, time/batch = 0.118\n",
      "700/2200 (epoch 15), train_loss = 1.813, time/batch = 0.116\n",
      "800/2200 (epoch 18), train_loss = 1.730, time/batch = 0.118\n",
      "900/2200 (epoch 20), train_loss = 1.680, time/batch = 0.124\n",
      "1000/2200 (epoch 22), train_loss = 1.627, time/batch = 0.119\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "1100/2200 (epoch 25), train_loss = 1.705, time/batch = 0.118\n",
      "1200/2200 (epoch 27), train_loss = 1.601, time/batch = 0.120\n",
      "1300/2200 (epoch 29), train_loss = 1.564, time/batch = 0.120\n",
      "1400/2200 (epoch 31), train_loss = 1.545, time/batch = 0.124\n",
      "1500/2200 (epoch 34), train_loss = 1.510, time/batch = 0.119\n",
      "1600/2200 (epoch 36), train_loss = 1.490, time/batch = 0.123\n",
      "1700/2200 (epoch 38), train_loss = 1.473, time/batch = 0.119\n",
      "1800/2200 (epoch 40), train_loss = 1.477, time/batch = 0.118\n",
      "1900/2200 (epoch 43), train_loss = 1.452, time/batch = 0.122\n",
      "2000/2200 (epoch 45), train_loss = 1.450, time/batch = 0.121\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "2100/2200 (epoch 47), train_loss = 1.433, time/batch = 0.120\n",
      "model saved to models_char_rnn/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "# let's train!\n",
    "train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chars_vocab.pkl                       model.ckpt-13000.data-00000-of-00001\r\n",
      "checkpoint                            model.ckpt-13000.index\r\n",
      "config.pkl                            model.ckpt-13000.meta\r\n",
      "model.ckpt-1099.data-00000-of-00001   model.ckpt-2199.data-00000-of-00001\r\n",
      "model.ckpt-1099.index                 model.ckpt-2199.index\r\n",
      "model.ckpt-1099.meta                  model.ckpt-2199.meta\r\n",
      "model.ckpt-11149.data-00000-of-00001  model.ckpt-4000.data-00000-of-00001\r\n",
      "model.ckpt-11149.index                model.ckpt-4000.index\r\n",
      "model.ckpt-11149.meta                 model.ckpt-4000.meta\r\n"
     ]
    }
   ],
   "source": [
    "# we're done with the model. the weights are now safe inside our storage\n",
    "%ls {args.save_dir}\n",
    "\n",
    "# so begone all the ops, graphs & variables!\n",
    "# you may ask, why is this line needed? try commenting out the line and see what happens later in the sampling stage\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalulation\n",
    "\n",
    "<font color=red>**Your model could be evaluated without traning procedure,**</font> if you saved and loaded your model properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=0\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "%env CUDA_DEVICE_ORDER = PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES = 0\n",
    "\n",
    "# load a bunch of libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.contrib import legacy_seq2seq\n",
    "import numpy as np\n",
    "import argparse\n",
    "import time\n",
    "import os\n",
    "from six.moves import cPickle\n",
    "from six import text_type\n",
    "import sys\n",
    "\n",
    "# this module is from the utils.py file of this folder\n",
    "# it handles loading texts to digits (aka. tokens) which are recognizable for the model\n",
    "from utils import TextLoader\n",
    "\n",
    "# this module is from the char_rnn.py file of this folder\n",
    "# the task is implementing the CharRNN inside the class definition from this file\n",
    "from char_rnn import Model\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# sampling definition for evaluation phase\n",
    "# it uses the saved model and spit out some characters from the RNN model\n",
    "def sample_eval(args):\n",
    "    with open(os.path.join(args.save_dir, 'config.pkl'), 'rb') as f:\n",
    "        saved_args = cPickle.load(f)\n",
    "    with open(os.path.join(args.save_dir, 'chars_vocab.pkl'), 'rb') as f:\n",
    "        chars, vocab = cPickle.load(f)\n",
    "    #Use most frequent char if no prime is given\n",
    "    if args.prime == '':\n",
    "        args.prime = chars[0]\n",
    "    model = Model(saved_args, training=False)\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        saver = tf.train.Saver(tf.global_variables())\n",
    "        ckpt = tf.train.get_checkpoint_state(args.save_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            print(str(model.sample(sess, chars, vocab, args.n, args.prime)),'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(n=500, prime='', save_dir='models_char_rnn')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# argparsing for sampling from the model\n",
    "parser_sample = argparse.ArgumentParser(\n",
    "                   formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "parser_sample.add_argument('--save_dir', type=str, default='models_char_rnn',\n",
    "                    help='model directory to store checkpointed models')\n",
    "parser_sample.add_argument('-n', type=int, default=500,\n",
    "                    help='number of characters to sample')\n",
    "parser_sample.add_argument('--prime', type=text_type, default=u'',\n",
    "                    help='prime text')\n",
    "sys.argv = ['-f']\n",
    "args_sample = parser_sample.parse_args()\n",
    "args_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models_char_rnn/model.ckpt-2199\n",
      " time of the mingt,\n",
      "I divind ment of bunk unking on fray we\n",
      "calls greet bloody eyes veint are done!\n",
      "Restion she should have dewither, for hither such raspish\n",
      "That allirate of to from the worn office.\n",
      "\n",
      "Firse Citizen:\n",
      "For such a queen tyard nouse unowant de; Art well;\n",
      "Who word have so itsel-and soveress a peace;\n",
      "Free hich my name eury awferumen's setfectaces!\n",
      "For his execorm appenous stait with thriugess'-breat?\n",
      "\n",
      "WARRY OF YORK:\n",
      "O, no protonce it, banish, and what hath.\n",
      "How plausestent but thou?\n",
      "'ri utf-8\n"
     ]
    }
   ],
   "source": [
    "# let's sample!\n",
    "sample_eval(args_sample)\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
